<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>第 5 章 卷积神经网络 | 深度学习与 TensorFlow 笔记</title>
  <meta name="description" content="第 5 章 卷积神经网络 | 深度学习与 TensorFlow 笔记>
  <meta name="generator" content="bookdown  and GitBook 2.6.7">

  <meta property="og:title" content="第 5 章 卷积神经网络 | 深度学习与 TensorFlow 笔记 />
  <meta property="og:type" content="book" />
  
  
  
  <meta name="github-repo" content="ruiqiangjiao/tf-note" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="第 5 章 卷积神经网络 | 深度学习与 TensorFlow 笔记 />
  
  
  




  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="nn.html">
<link rel="next" href="rnn.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />










<link rel="stylesheet" href="css\style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">深度学习以及 TensorFlow 笔记</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> TensorFlow 简介</a></li>
<li class="chapter" data-level="2" data-path="install.html"><a href="install.html"><i class="fa fa-check"></i><b>2</b> 安装</a><ul>
<li class="chapter" data-level="2.1" data-path="install.html"><a href="install.html#section"><i class="fa fa-check"></i><b>2.1</b> 安装系统</a></li>
<li class="chapter" data-level="2.2" data-path="install.html"><a href="install.html#section-1"><i class="fa fa-check"></i><b>2.2</b> 安装步骤</a></li>
<li class="chapter" data-level="2.3" data-path="install.html"><a href="install.html#section-2"><i class="fa fa-check"></i><b>2.3</b> 参考资料</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="basic.html"><a href="basic.html"><i class="fa fa-check"></i><b>3</b> 基础知识</a><ul>
<li class="chapter" data-level="3.1" data-path="basic.html"><a href="basic.html#tensorflow--"><i class="fa fa-check"></i><b>3.1</b> TensorFlow 计算模型—— 计算图</a><ul>
<li class="chapter" data-level="3.1.1" data-path="basic.html"><a href="basic.html#section-3"><i class="fa fa-check"></i><b>3.1.1</b> 计算图</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="basic.html"><a href="basic.html#tensorflow---1"><i class="fa fa-check"></i><b>3.2</b> TensorFlow 数据模型 —— 张量</a></li>
<li class="chapter" data-level="3.3" data-path="basic.html"><a href="basic.html#tensorflow---2"><i class="fa fa-check"></i><b>3.3</b> TensorFlow 运行模型 —— 会话</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="nn.html"><a href="nn.html"><i class="fa fa-check"></i><b>4</b> 神经网络</a><ul>
<li class="chapter" data-level="4.1" data-path="nn.html"><a href="nn.html#section-4"><i class="fa fa-check"></i><b>4.1</b> 研究背景(解决的问题、应用场景、发展过程)</a></li>
<li class="chapter" data-level="4.2" data-path="nn.html"><a href="nn.html#section-5"><i class="fa fa-check"></i><b>4.2</b> 内容</a><ul>
<li class="chapter" data-level="4.2.1" data-path="nn.html"><a href="nn.html#section-6"><i class="fa fa-check"></i><b>4.2.1</b> 神经元</a></li>
<li class="chapter" data-level="4.2.2" data-path="nn.html"><a href="nn.html#section-7"><i class="fa fa-check"></i><b>4.2.2</b> 神经网络</a></li>
<li class="chapter" data-level="4.2.3" data-path="nn.html"><a href="nn.html#section-8"><i class="fa fa-check"></i><b>4.2.3</b> 训练过程存在的问题</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="nn.html"><a href="nn.html#section-9"><i class="fa fa-check"></i><b>4.3</b> 实例分析</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="cnn.html"><a href="cnn.html"><i class="fa fa-check"></i><b>5</b> 卷积神经网络</a><ul>
<li class="chapter" data-level="5.1" data-path="cnn.html"><a href="cnn.html#section-10"><i class="fa fa-check"></i><b>5.1</b> 研究背景以及发展历史</a></li>
<li class="chapter" data-level="5.2" data-path="cnn.html"><a href="cnn.html#section-11"><i class="fa fa-check"></i><b>5.2</b> 所要解决的问题</a></li>
<li class="chapter" data-level="5.3" data-path="cnn.html"><a href="cnn.html#section-12"><i class="fa fa-check"></i><b>5.3</b> 卷积神经网络如何解决这些问题?</a><ul>
<li class="chapter" data-level="5.3.1" data-path="cnn.html"><a href="cnn.html#lenet-5"><i class="fa fa-check"></i><b>5.3.1</b> LeNet-5</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="rnn.html"><a href="rnn.html"><i class="fa fa-check"></i><b>6</b> 循环神经网络</a><ul>
<li class="chapter" data-level="6.1" data-path="rnn.html"><a href="rnn.html#rnn-1"><i class="fa fa-check"></i><b>6.1</b> 循环神经网络(RNN)</a><ul>
<li class="chapter" data-level="6.1.1" data-path="rnn.html"><a href="rnn.html#section-13"><i class="fa fa-check"></i><b>6.1.1</b> 循环神经网络的训练</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="rnn.html"><a href="rnn.html#lstm"><i class="fa fa-check"></i><b>6.2</b> 长短时记忆网络(LSTM)</a></li>
<li class="chapter" data-level="6.3" data-path="rnn.html"><a href="rnn.html#word2vec"><i class="fa fa-check"></i><b>6.3</b> Word2Vec</a><ul>
<li class="chapter" data-level="6.3.1" data-path="rnn.html"><a href="rnn.html#cbow"><i class="fa fa-check"></i><b>6.3.1</b> CBOW</a></li>
<li class="chapter" data-level="6.3.2" data-path="rnn.html"><a href="rnn.html#skip-gram"><i class="fa fa-check"></i><b>6.3.2</b> Skip-Gram</a></li>
</ul></li>
</ul></li>
<li class="appendix"><span><b>附录</b></span></li>
<li class="chapter" data-level="A" data-path="deep-learning.html"><a href="deep-learning.html"><i class="fa fa-check"></i><b>A</b> 深度学习</a><ul>
<li class="chapter" data-level="A.1" data-path="deep-learning.html"><a href="deep-learning.html#section-14"><i class="fa fa-check"></i><b>A.1</b> 激活函数区线性化</a></li>
<li class="chapter" data-level="A.2" data-path="deep-learning.html"><a href="deep-learning.html#section-15"><i class="fa fa-check"></i><b>A.2</b> 多层神经网络解决亦或问题</a></li>
<li class="chapter" data-level="A.3" data-path="deep-learning.html"><a href="deep-learning.html#section-16"><i class="fa fa-check"></i><b>A.3</b> 卷积</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://bookdown.org" target="blank">深度学习以及 TensorFlow 笔记</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">深度学习与 TensorFlow 笔记</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="cnn" class="section level1">
<h1><span class="header-section-number">第 5 章</span> 卷积神经网络</h1>
<div id="section-10" class="section level2">
<h2><span class="header-section-number">5.1</span> 研究背景以及发展历史</h2>
<p>20 世纪60 年代,美国神经生物学家 Hubel 和 Wiesel 在研究猫脑皮层中用于局部敏感和方向选择的神经元时发现其独特的网络结构可以有效地降低反馈神经网络的复杂性,日本人福岛邦彦（ Kunihiko Fukushima）在 20 个世纪 90 年代提出的感知机是卷积神经网络的第一个实现网络.随后,更多的科研工作者对该网络进行了改进.</p>
<div class="figure" style="text-align: center"><span id="fig:cnn-hist"></span>
<img src="images/cnn-history.jpg" alt="卷积神经网络结构演化的历史" width="1514" />
<p class="caption">
图 5.1: 卷积神经网络结构演化的历史
</p>
</div>
</div>
<div id="section-11" class="section level2">
<h2><span class="header-section-number">5.2</span> 所要解决的问题</h2>
<p>参数数量太多, 收敛速度慢,对于动辄处理百万维度的图像分类任务,由于找不到计算速度能够满足需要的处理器,因此这样的需求变得不可行.</p>
</div>
<div id="section-12" class="section level2">
<h2><span class="header-section-number">5.3</span> 卷积神经网络如何解决这些问题?</h2>
<ul>
<li><p>局部连接</p></li>
<li><p>权值共享</p></li>
<li><p>下采样</p></li>
</ul>

<div class="rmdnote">
<p>在卷积核对前面输入的这一层数据向量进行扫描的时候,需要对周围进行填充(Padding),主要有两个原因:</p>
<p>1 保持边界信息.因为如果不加Padding的话,最边缘的像素点信息其实仅仅被卷积核扫描了一遍，而图像中间的像素点信息会被扫描多遍,在一定程度上等于降低了边界上信息的参考程度.Padding后就可以在一定程度上解决这个问题.在实际处理的过程中肯定是Padding了一些0值以后,再从Padding后的新边界开始扫描.</p>
2 保持边界信息.因为如果不加Padding的话,最边缘的像素点信息其实仅仅被卷积核扫描了一遍,而图像中间的像素点信息会被扫描多遍，在一定程度上等于降低了边界上信息的参考程度.Padding后就可以在一定程度上解决这个问题.在实际处理的过程中肯定是Padding 了一些0 值以后,再从Padding 后的新边界开始扫描.
</div>

<p>卷积神经网络构建由输入层、卷积层、池化层、全连接层和Softmax层组成,具体地可参考<a href="https://blog.csdn.net/yunpiao123456/article/details/52437794">资料</a>.</p>
<div id="lenet-5" class="section level3">
<h3><span class="header-section-number">5.3.1</span> LeNet-5</h3>
<p><strong>LeNet-5</strong> 模型是 Yann LeCun 教授于 1998 年 在论文 <a href="http://yann.lecun.com/exdb/publis/pdf/lecun-98.pdf">Gradient-BasedLearning Applied to Document Recognition</a><a href="#fn1" class="footnote-ref" id="fnref1"><sup>1</sup></a> 中提出来的.</p>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-5"></span>
<img src="images/lenet5.png" alt="LeNet5 模型结构图" width="824" />
<p class="caption">
图 5.2: LeNet5 模型结构图
</p>
</div>
<ul>
<li>输入层</li>
</ul>
<p>    <span class="math inline">\(32 \times 32\)</span> 灰度值图像</p>
<ul>
<li><span class="math inline">\(C_1\)</span> 卷积层</li>
</ul>
<p>    对输入图像进行卷积运算(使用 <span class="math inline">\(6\)</span> 个大小为 <span class="math inline">\(5 \times 5\)</span> 的卷积核), 得到 <span class="math inline">\(6\)</span> 个 <span class="math inline">\(C_1\)</span> 特征图( <span class="math inline">\(6\)</span> 个大小为 <span class="math inline">\(28 \times 28\)</span> 的 Feature Maps , <span class="math inline">\(32-5+1=28\)</span> ).总共就有 <span class="math inline">\(6 \times (5 \times 5 + 1)=156\)</span> 个参数,对于卷积层 <span class="math inline">\(C_1\)</span> 总共有 <span class="math inline">\(156 \times 28 \times 28=122304\)</span> 个连接.</p>
<ul>
<li><span class="math inline">\(S_2\)</span> 池化层</li>
</ul>
<p>    使用 <span class="math inline">\(2 \times 2\)</span> 过滤器进行池化,长和宽池化的步长为 <span class="math inline">\(2\)</span>, <span class="math inline">\(2 \times 2\)</span> 单元里的值相加然后再乘以训练参数 <span class="math inline">\(w\)</span>, 再加上一个偏置参数 <span class="math inline">\(b\)</span> (每一个 feature map 共享相同 <span class="math inline">\(w\)</span> 和 <span class="math inline">\(b\)</span> ), 然后取 <strong>sigmoid</strong> 值, 作为对应的该单元的值.</p>
<ul>
<li><span class="math inline">\(C_3\)</span> 卷积层</li>
</ul>
<p>    使用 <span class="math inline">\(16\)</span> 个大小为 <span class="math inline">\(5 \times 5\)</span> 的卷积核进行卷积运算, 因此具有 <span class="math inline">\(16\)</span> 个 Feature Maps, 每个 Feature Maps 的大小为 <span class="math inline">\((14-5+1) \times (14-5+1) = 10 \times 10\)</span>. 每个 Feature Maps 只与上一层 <span class="math inline">\(S_2\)</span> 中部分 Feature Maps 相连接, 下图 <a href="cnn.html#fig:c3">5.3</a> 给出了 <span class="math inline">\(16\)</span> 个 Feature Maps 与上一层 <span class="math inline">\(S_2\)</span> 的连接方式(行为 <span class="math inline">\(S_2\)</span> 层 Feature Maps 的标号, 列为<span class="math inline">\(C_3\)</span> 层 Feature Maps 的标号,第一列表示 <span class="math inline">\(C_3\)</span> 层的第 <span class="math inline">\(0\)</span> 个 Feature Maps 只有 <span class="math inline">\(S_2\)</span> 层的第<span class="math inline">\(0、1\)</span>和 <span class="math inline">\(2\)</span> 这三个 Feature Maps 相连接, 其它类似). 采用部分连接, 主要原因有:</p>
<p>    1 减少参数;</p>
<p>    2 打破对称性，这样就能得到输入的不同特征集合.</p>
<div class="figure" style="text-align: center"><span id="fig:c3"></span>
<img src="images/lenet5_c3.png" alt="卷积层3的连接方式" width="440" />
<p class="caption">
图 5.3: 卷积层3的连接方式
</p>
</div>
<p>以第 <span class="math inline">\(0\)</span> 个 Feature Maps 描述计算过程: 用<span class="math inline">\(1\)</span>个卷积核(对应 <span class="math inline">\(3\)</span> 个卷积模板,但仍称为一个卷积核,可以认为是三维卷积核)分别与 <span class="math inline">\(S_2\)</span> 层的 <span class="math inline">\(3\)</span> 个 Feature Maps 进行卷积, 然后将卷积的结果相加,再加上一个偏置，再取 <strong>sigmoid</strong> 就可以得出对应的 Feature Maps 了。所需要的参数数目为<span class="math display">\[(5 \times 5 \times 3 + 1) \times 6+(5 \times 5 \times 4 + 1 ) \times 9 +5 \times 5 \times 6 + 1 = 1516\]</span>
<span class="math inline">\(5 \times5\)</span> 为卷积参数,卷积核分别有 <span class="math inline">\(3 、4、 6\)</span> 个卷积模板, 连接数为<span class="math inline">\(1516 \times 10 \times 10= 151600\)</span> .</p>
<p><span class="math inline">\(C_3\)</span> 与 <span class="math inline">\(S_2\)</span> 中前 <span class="math inline">\(3\)</span> 个图相连的卷积结构如下图 <a href="cnn.html#fig:c31">5.4</a> 所示:</p>
<div class="figure" style="text-align: center"><span id="fig:c31"></span>
<img src="images/s2_c3.png" alt="C3与S2中前3个图相连的卷积结构图" width="292" />
<p class="caption">
图 5.4: C3与S2中前3个图相连的卷积结构图
</p>
</div>
<ul>
<li><span class="math inline">\(S_4\)</span> 池化层</li>
</ul>
<p>过程和 <span class="math inline">\(S_2\)</span>类似.</p>
<ul>
<li><span class="math inline">\(C_5\)</span> 卷积层</li>
</ul>
<p>    使用 <span class="math inline">\(16\)</span> 个大小为 <span class="math inline">\(5 \times 5\)</span> 的卷积核进行卷积运算, 由于 <span class="math inline">\(S_5\)</span> 的图像和卷积核的大小一样, 因此进行的是全连接. 生成 <span class="math inline">\(120\)</span> 个元素的向量,每一个向量的生成过程如下:</p>
<div class="figure" style="text-align: center"><span id="fig:c51"></span>
<img src="images/c5.png" alt="C5与S4卷积结构图" width="292" />
<p class="caption">
图 5.5: C5与S4卷积结构图
</p>
</div>
<ul>
<li><span class="math inline">\(F_6\)</span> 全连接层</li>
</ul>
<p>    计算方式:输入向量与权重向量进行点积运算, 在加上一个偏置项,最后通过 <strong>sigmoid</strong> 函数进行映射.</p>
<p>    说明: <span class="math inline">\(F_6\)</span> 层有 <span class="math inline">\(84\)</span> 个节点, 对应于一个 <span class="math inline">\(7 \times 12\)</span> 的比特图, 这样每个符号的比特图的黑白色就对应于一个编码.</p>
<div class="figure" style="text-align: center"><span id="fig:bite"></span>
<img src="images/bite.png" alt="比特图" width="298" />
<p class="caption">
图 5.6: 比特图
</p>
</div>
<ul>
<li><span class="math inline">\(F_7\)</span> 输出层</li>
</ul>
<p>输出层共有 <span class="math inline">\(10\)</span> 个节点, 分别代表着数字<span class="math inline">\(0 ~ 9\)</span>,且如果节点 <span class="math inline">\(i\)</span> 的值为 <span class="math inline">\(0\)</span>,则网络识别的结果是数字<span class="math inline">\(i\)</span>.
采用的是径向基函数 <span class="math inline">\((RBF)\)</span> 的网络连接方式.则 <strong>RBF</strong> 输出的计算方式是:</p>
<p><span class="math display">\[y_i = \sum_j (x_j - w_{ij})^2\]</span></p>
<p>下图 <a href="cnn.html#fig:t3">5.7</a> 是 <strong>LeNet-5</strong> 识别数字 <span class="math inline">\(3\)</span> 的过程.</p>
<div class="figure" style="text-align: center"><span id="fig:t3"></span>
<img src="images/three_prpgress.png" alt="LeNet-5识别数字3的过程" width="373" />
<p class="caption">
图 5.7: LeNet-5识别数字3的过程
</p>
</div>
<p>参考资料</p>
<p><a href="http://www.cnblogs.com/ooon/p/5415888.html" class="uri">http://www.cnblogs.com/ooon/p/5415888.html</a></p>
<p><a href="https://blog.csdn.net/happyer88/article/details/46762919" class="uri">https://blog.csdn.net/happyer88/article/details/46762919</a></p>
<p><a href="https://blog.csdn.net/zhongkeli/article/details/51854619" class="uri">https://blog.csdn.net/zhongkeli/article/details/51854619</a></p>
<p><a href="http://cuijiahua.com/blog/2018/01/dl_3.html" class="uri">http://cuijiahua.com/blog/2018/01/dl_3.html</a></p>

</div>
</div>
</div>
<div class="footnotes">
<hr />
<ol start="1">
<li id="fn1"><p>Lecun Y, Bottou L, Bengio Y, et al. Gradient-based learning applied to document recognition[J]. Proceedings of the IEEE, 1998, 86(11):2278-2324.<a href="cnn.html#fnref1" class="footnote-back">↩</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="nn.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="rnn.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": true,
"facebook": false,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/ruiqiangJiao/tf-note/edit/master/04-cnn.Rmd",
"text": "缂栬緫"
},
"history": {
"link": null,
"text": null
},
"download": ["docs.pdf", "docs.epub"],
"toc": {
"collapse": "none"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(src))
      src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
