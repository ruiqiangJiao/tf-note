<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>第 4 章 神经网络 | 深度学习与 TensorFlow 笔记</title>
  <meta name="description" content="第 4 章 神经网络 | 深度学习与 TensorFlow 笔记>
  <meta name="generator" content="bookdown  and GitBook 2.6.7">

  <meta property="og:title" content="第 4 章 神经网络 | 深度学习与 TensorFlow 笔记 />
  <meta property="og:type" content="book" />
  
  
  
  <meta name="github-repo" content="ruiqiangjiao/tf-note" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="第 4 章 神经网络 | 深度学习与 TensorFlow 笔记 />
  
  
  




  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="basic.html">
<link rel="next" href="cnn.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />










<link rel="stylesheet" href="css\style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">深度学习以及 TensorFlow 笔记</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> TensorFlow 简介</a></li>
<li class="chapter" data-level="2" data-path="install.html"><a href="install.html"><i class="fa fa-check"></i><b>2</b> 安装</a><ul>
<li class="chapter" data-level="2.1" data-path="install.html"><a href="install.html#section"><i class="fa fa-check"></i><b>2.1</b> 安装系统</a></li>
<li class="chapter" data-level="2.2" data-path="install.html"><a href="install.html#section-1"><i class="fa fa-check"></i><b>2.2</b> 安装步骤</a></li>
<li class="chapter" data-level="2.3" data-path="install.html"><a href="install.html#section-2"><i class="fa fa-check"></i><b>2.3</b> 参考资料</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="basic.html"><a href="basic.html"><i class="fa fa-check"></i><b>3</b> 基础知识</a><ul>
<li class="chapter" data-level="3.1" data-path="basic.html"><a href="basic.html#tensorflow--"><i class="fa fa-check"></i><b>3.1</b> TensorFlow 计算模型—— 计算图</a><ul>
<li class="chapter" data-level="3.1.1" data-path="basic.html"><a href="basic.html#section-3"><i class="fa fa-check"></i><b>3.1.1</b> 计算图</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="basic.html"><a href="basic.html#tensorflow---1"><i class="fa fa-check"></i><b>3.2</b> TensorFlow 数据模型 —— 张量</a></li>
<li class="chapter" data-level="3.3" data-path="basic.html"><a href="basic.html#tensorflow---2"><i class="fa fa-check"></i><b>3.3</b> TensorFlow 运行模型 —— 会话</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="nn.html"><a href="nn.html"><i class="fa fa-check"></i><b>4</b> 神经网络</a><ul>
<li class="chapter" data-level="4.1" data-path="nn.html"><a href="nn.html#section-4"><i class="fa fa-check"></i><b>4.1</b> 研究背景(解决的问题、应用场景、发展过程)</a></li>
<li class="chapter" data-level="4.2" data-path="nn.html"><a href="nn.html#section-5"><i class="fa fa-check"></i><b>4.2</b> 内容</a><ul>
<li class="chapter" data-level="4.2.1" data-path="nn.html"><a href="nn.html#section-6"><i class="fa fa-check"></i><b>4.2.1</b> 神经元</a></li>
<li class="chapter" data-level="4.2.2" data-path="nn.html"><a href="nn.html#section-7"><i class="fa fa-check"></i><b>4.2.2</b> 神经网络</a></li>
<li class="chapter" data-level="4.2.3" data-path="nn.html"><a href="nn.html#section-8"><i class="fa fa-check"></i><b>4.2.3</b> 训练过程存在的问题</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="nn.html"><a href="nn.html#section-9"><i class="fa fa-check"></i><b>4.3</b> 实例分析</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="cnn.html"><a href="cnn.html"><i class="fa fa-check"></i><b>5</b> 卷积神经网络</a><ul>
<li class="chapter" data-level="5.1" data-path="cnn.html"><a href="cnn.html#section-10"><i class="fa fa-check"></i><b>5.1</b> 研究背景以及发展历史</a></li>
<li class="chapter" data-level="5.2" data-path="cnn.html"><a href="cnn.html#section-11"><i class="fa fa-check"></i><b>5.2</b> 所要解决的问题</a></li>
<li class="chapter" data-level="5.3" data-path="cnn.html"><a href="cnn.html#section-12"><i class="fa fa-check"></i><b>5.3</b> 卷积神经网络如何解决这些问题?</a><ul>
<li class="chapter" data-level="5.3.1" data-path="cnn.html"><a href="cnn.html#lenet-5"><i class="fa fa-check"></i><b>5.3.1</b> LeNet-5</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="rnn.html"><a href="rnn.html"><i class="fa fa-check"></i><b>6</b> 循环神经网络</a><ul>
<li class="chapter" data-level="6.1" data-path="rnn.html"><a href="rnn.html#rnn-1"><i class="fa fa-check"></i><b>6.1</b> 循环神经网络(RNN)</a><ul>
<li class="chapter" data-level="6.1.1" data-path="rnn.html"><a href="rnn.html#section-13"><i class="fa fa-check"></i><b>6.1.1</b> 循环神经网络的训练</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="rnn.html"><a href="rnn.html#lstm"><i class="fa fa-check"></i><b>6.2</b> 长短时记忆网络(LSTM)</a></li>
<li class="chapter" data-level="6.3" data-path="rnn.html"><a href="rnn.html#word2vec"><i class="fa fa-check"></i><b>6.3</b> Word2Vec</a><ul>
<li class="chapter" data-level="6.3.1" data-path="rnn.html"><a href="rnn.html#cbow"><i class="fa fa-check"></i><b>6.3.1</b> CBOW</a></li>
<li class="chapter" data-level="6.3.2" data-path="rnn.html"><a href="rnn.html#skip-gram"><i class="fa fa-check"></i><b>6.3.2</b> Skip-Gram</a></li>
</ul></li>
</ul></li>
<li class="appendix"><span><b>附录</b></span></li>
<li class="chapter" data-level="A" data-path="deep-learning.html"><a href="deep-learning.html"><i class="fa fa-check"></i><b>A</b> 深度学习</a><ul>
<li class="chapter" data-level="A.1" data-path="deep-learning.html"><a href="deep-learning.html#section-14"><i class="fa fa-check"></i><b>A.1</b> 激活函数区线性化</a></li>
<li class="chapter" data-level="A.2" data-path="deep-learning.html"><a href="deep-learning.html#section-15"><i class="fa fa-check"></i><b>A.2</b> 多层神经网络解决亦或问题</a></li>
<li class="chapter" data-level="A.3" data-path="deep-learning.html"><a href="deep-learning.html#section-16"><i class="fa fa-check"></i><b>A.3</b> 卷积</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://bookdown.org" target="blank">深度学习以及 TensorFlow 笔记</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">深度学习与 TensorFlow 笔记</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="nn" class="section level1">
<h1><span class="header-section-number">第 4 章</span> 神经网络</h1>
<div id="section-4" class="section level2">
<h2><span class="header-section-number">4.1</span> 研究背景(解决的问题、应用场景、发展过程)</h2>
</div>
<div id="section-5" class="section level2">
<h2><span class="header-section-number">4.2</span> 内容</h2>
<div id="section-6" class="section level3">
<h3><span class="header-section-number">4.2.1</span> 神经元</h3>
<p>神经元是神经网络基本构成单元，它主要由线性模型和激活函数两部分构成。如下图所示：</p>
<p><img src="images/nns.png" /></p>
<p>常用的激活函数：</p>
<ul>
<li>Sigmoid 函数</li>
</ul>
<p><span class="math display" id="eq:sigmoid">\[\begin{equation} 
  f(x) = \frac{1}{1+e^{-(wx+b)}}
  \tag{4.1}
\end{equation}\]</span></p>
<p><img src="images/sigmoid.png" /></p>
<ul>
<li><p>Tanh 函数</p>
<pre><code>$$f(x) = \frac{e^{x}-e^{-x}}{e^{x}+e^{-x}}$$</code></pre></li>
<li><p>ReLU 函数</p>
<p><span class="math display">\[f(x) = max(0 , x)\]</span></p></li>
</ul>
<p><img src="images/nnp2.png" /></p>
<p>如上图，神经元就是通过通过这种首尾相连的方式进行信息传递，前一个神经元接受数据，经过处理传递给下一层的一个或者多个神经元。一旦多个神经元首尾连接形成一个类似网络的结构来协同工作的时候，那就可以被称为神经网络了。</p>
</div>
<div id="section-7" class="section level3">
<h3><span class="header-section-number">4.2.2</span> 神经网络</h3>
<p>神经网络主要的思想:提取输入特征变量的线性组合作为衍生特征，然后对这些衍生特征进行非线性建模。单层感知机（单隐层反向传播网络）如下图:</p>
<p><img src="images/nn_progrecess.png" /></p>
<p>数学表达式为:</p>
<p><span class="math display">\[\begin{align} 
Z_m &amp;= \sigma(\alpha_{0m} + \alpha_m^T X) , m = 1 , \dots , M,\\
T_k &amp;= \beta_{0k}+ \beta_k^{T}Z , k = 1, \dots , K,\\
f_k(X) &amp;= g_k(T) , k = 1, \dots , K,
\end{align}\]</span></p>
<p>通常隐藏层的激活函数<span class="math inline">\(\sigma(\cdot)\)</span> 选择 Sigmoid 函数<a href="nn.html#eq:sigmoid">(4.1)</a>,有时也选择高斯径向基函数,当然选择高斯径向基函数时得到的模型被称为径向基传播神经网络.对于回归模型,输出层的激活函数选择恒等函数,即<span class="math inline">\(g_k(T) = T_k\)</span>,对于分类模型,输出层的激活函数选择 SoftMax 函数,即</p>
<p><span class="math display">\[g_k(T) = \frac{e^{T_k}}{\sum_{k =1}^{K}e^{T_k}}\]</span></p>

<div class="rmdtip">
<p>神经网络输出为n个类别的向量<span class="math inline">\([1 , 0 , 0 , \cdots , 0]\)</span>,如何度量向量与期望向量之间的距离,通过交叉熵来判定，而交叉熵度量的是两个概率分布之间的距离，因此需要将输出结果转化成概率,经过softmax回归函数转化之后的输出表示一个样例为不同类别的概率有多大.</p>
</div>

<p>现在我们已经建立起模型了，但是模型中存在未知参数需要通过训练数据来进行学习。学习参数的过程即为拟合模型过程,神经网络模型的效果以及优化的目标是通过损失函数来反应.</p>
<ul>
<li><p>分类</p>
<pre><code>- 交叉熵: 度量两个概率分布之间的距离.
      $$R(\theta) = - \sum_{i = 1}^{N} \sum_{k =1}^{K} y_{ik}logf_k(x_i)$$</code></pre></li>
<li><p>回归</p>
<pre><code>- 均方误差
$$R(\theta) = \sum_{k =1}^{K} \sum_{i = 1}^{N} (y_{ik} - f_k(x_i))^2$$</code></pre></li>
</ul>
<p>如何得到使损失函数达到最小的模型参数?</p>
<ul>
<li>迭代法</li>
</ul>
<p>迭代法的核心思路就是用步步逼近的方式来接近理论上的精确值,只要发现当前的试探值已经收敛到一个满足场景要求的误差精度就可以判断迭代结束,用这个试探值来充当求解的目标值.</p>
<ul>
<li>梯度下降法</li>
</ul>
<p><span class="math display">\[\begin{align}
\nabla &amp;= (\frac{\partial R}{\partial \beta_1} , \dots , \frac{\partial R}{\partial \beta_K}) \\
\beta_n^k  &amp;=  \beta_{n-1}^k - \eta \frac{\partial R}{\partial \beta^k}
\end{align}\]</span></p>
</div>
<div id="section-8" class="section level3">
<h3><span class="header-section-number">4.2.3</span> 训练过程存在的问题</h3>
<ul>
<li><p>参数初始化</p>
<pre><code>- 以0为均值,很小的值为标准差的正态随机数;

- 以0 为均值μ 、以1 为方差σ 的分布生成后除以当前层的神经元个数的算术平方根;</code></pre>
<p>输入特征的权重相当于是一种重视程度或者采纳程度的表示,而在一个模型中那些对判断结果需要作为非常重要的正面因素采纳的是少数，需要作为非常重要的负面因素采纳的也是少数，而其他大部分输入的信息可能就是那些比较中庸的,不论一个单体数据的分布特点如何，在大量单体数据叠加后的宏观数据表现都会呈现出高斯分布的特点,中心极限定理反应的实质.</p></li>
<li><p>学习率</p>
<p>如果过大的话,参数在极优值的两侧来回移动,不满足收敛性,若过小的话,计算成本较高.指数衰减法,先以快速收敛,后在收敛点稳定下来.</p></li>
<li><p>层数和每层单元数</p>
<p>每层单元数 50 ~ 100</p></li>
<li><p>梯度消失、梯度爆破</p></li>
</ul>
<p>这种问题的发生会让训练很难进行下去,看到的现象就是训练不再收敛一 Loss 过早地不再下降，
而精确度也过早地不再提高.</p>
<ul>
<li>优化到什么程度就可以？防止过拟合。过拟合解决方法 —— 正则化</li>
</ul>
</div>
</div>
<div id="section-9" class="section level2">
<h2><span class="header-section-number">4.3</span> 实例分析</h2>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="basic.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="cnn.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": true,
"facebook": false,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/ruiqiangJiao/tf-note/edit/master/03-nn.Rmd",
"text": "缂栬緫"
},
"history": {
"link": null,
"text": null
},
"download": ["docs.pdf", "docs.epub"],
"toc": {
"collapse": "none"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(src))
      src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
